# Human-Action-Recognition-From-Depth-Maps-And-Postures-Using-Deep-Learning
Human Activity Recognition is one of the active research areas in computer vision for various contexts like security surveillance, healthcare and human computer interaction.

Over the past years, several methods published for human action recognition using RGB (red, green, and blue), depth, and skeleton datasets. Most of the methods introduced for action classification using skeleton datasets are constrained in some perspectives including features representation, complexity, and performance. However, there is still a challenging problem of providing an effective and efficient method for human action discrimination using a skeleton dataset. The first input is depth images and second input is a proposed moving joints descriptor (MJD) which represents the motion of body joints over time, in order to maximize feature extraction for accurate action classification, CNN channels are trained with different inputs and for Score fusion we are planning to use neural networks. Our proposed method was implementation on public datasets like MSRAction3D.

Human action recognition has been a popular research topic for a long time, not only because of their widespread application in a variety of applications, such as intelligent surveillance systems, human-robot interaction, and home care systems, but also because it remains a difficult research problem. Convolutional Neural Networks (CNN) have been widely used in many research areas, particularly computer vision and pattern recognition, thanks to the advancement of deep learning over the last few years, and have achieved remarkable performance on classification, detection, segmentation, and tracking tasks. In the subject of action recognition, there are a few things to keep in mind. As human-machine interaction becomes one of the most researched topics in multimedia processing, traditional communication techniques are being developed in order to address technological advancements and enable disabled people to communicate with machines and understand their activities using computer computing. Many research works have attempted to model and then recognise people’s behavior using motion analysis. In this paper, we focus on human behaviour analysis from video scenes, and it is worth noting that many information is hid behind gesture, rapid motion, and walking pace[1],[2].Due to the expressive features provided by the two types of data, recent human action recognition research has been directed toward using depth maps or body postures to represent the action[3]. A strong representation that gives distinct aspects of each action for classification is critical to the success of an action recognition algorithm. For some activities, using depth map data for action recognition is still confusing, resulting in incorrect classification because two actions may appear similar from the front, but they appear differently from the side[4].When substantial occlusions occur, the depth maps collected by the depth cameras are quite noisy, and the 3D positions of the tracked joints may be completely incorrect, increasing intraclass variances in the actions[5].They propose ”Skepxels,” a spatio-temporal format for skeletal sequences that uses CNN’s 2D convolution kernels to fully leverage ”local” connections between joints. Thye use Skepxels to convert skeleton movies into images with adjustable dimensions, and then use the generated images to build a CNN-based framework for successful human action recognition.
